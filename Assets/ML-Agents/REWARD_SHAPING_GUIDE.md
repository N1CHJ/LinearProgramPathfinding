# ğŸ¯ Reward Shaping Guide - CubeSat Navigation

## ğŸš¨ Problem You Identified (270k steps)

**Issue:** Agent learns to "stay alive" instead of "reach goal"

**Why:**
```
Time penalty over 30s: -0.001 Ã— ~1500 steps = -1.5
Collision penalty: -1.0
Boundary penalty: -1.0
Timeout penalty: -0.5
Goal reward: +1.0

Agent thinks:
"Avoiding crashes (-1.0) is as good as reaching goal (+1.0)!"
"Staying alive longer = less penalty per second"
```

**Result:** Agent circles around, avoiding asteroids, not pursuing goal âŒ

---

## âœ… NEW REWARD STRUCTURE (Balanced & Smart)

### **Configurable Reward Settings (NEW!):**

```
Select: CubeSat â†’ Inspector â†’ Reward Settings

Goal Reward: 2.0                    â† DOUBLED! Goal is now 2Ã— better
Collision Penalty: -1.0             â† Same (crash = bad)
Timeout Penalty: -1.0               â† CHANGED from -0.5
Time Step Penalty: -0.001           â† Same (small per-step cost)
Max Speed At Goal: 3.0              â† NEW! Safe arrival speed
Speed Penalty Multiplier: 0.1       â† NEW! Penalty for fast arrival
Excessive Speed Penalty: 0.0001     â† NEW! Tiny penalty for going too fast
```

---

## ğŸ“Š Reward Breakdown

### **1. Goal Reward (Success!)**
```
Base: +2.0

Speed at arrival:
â”œâ”€â”€ Speed â‰¤ 3.0 m/s: Full +2.0 reward âœ… (safe landing!)
â”œâ”€â”€ Speed = 5.0 m/s: +2.0 - (2.0 Ã— 0.1) = +1.8
â”œâ”€â”€ Speed = 10.0 m/s: +2.0 - (7.0 Ã— 0.1) = +1.3 (too fast!)
â””â”€â”€ Speed = 20.0 m/s: +2.0 - (17.0 Ã— 0.1) = +0.3 (crash speed!)

Formula:
reward = goalReward - (excessSpeed Ã— speedPenaltyMultiplier)
where excessSpeed = max(0, currentSpeed - maxSpeedAtGoal)
```

**Effect:** Encourages controlled, safe arrival! ğŸ›°ï¸

---

### **2. Time-Based Penalties**

#### **Per-Step Penalty:**
```
-0.001 per FixedUpdate step

Over 30 seconds (~1500 steps):
Total = -0.001 Ã— 1500 = -1.5

Over 60 seconds (~3000 steps):
Total = -0.001 Ã— 3000 = -3.0  â† Way worse than timeout!
```

#### **Timeout Penalty (NEW!):**
```
If episode reaches maxEpisodeTime (30s):
Penalty = -1.0  â† NOW EQUALS collision/boundary penalties!

Total if timing out:
= timeStepPenalty Ã— steps + timeoutPenalty
= (-0.001 Ã— 1500) + (-1.0)
= -1.5 + -1.0
= -2.5  â† Worse than collision (-1.0)!
```

**Effect:** Timing out is now worse than crashing â†’ Agent motivated to take action!

---

### **3. Failure Penalties (Equal Weight)**

```
Collision with asteroid: -1.0
Leaving boundary: -1.0
Timeout (30s): -1.0 (via timeoutPenalty)

All failures now equal!
Agent doesn't prefer "survival" over "action"
```

---

### **4. Excessive Speed Penalty (NEW!)**

```
If speed > 15 m/s during flight:
penalty = -0.0001 Ã— (speed - 15)

Examples:
â”œâ”€â”€ Speed = 10 m/s: No penalty âœ“
â”œâ”€â”€ Speed = 15 m/s: No penalty âœ“
â”œâ”€â”€ Speed = 20 m/s: -0.0001 Ã— 5 = -0.0005 per step
â”œâ”€â”€ Speed = 30 m/s: -0.0001 Ã— 15 = -0.0015 per step
â””â”€â”€ Speed = 50 m/s: -0.0001 Ã— 35 = -0.0035 per step

If going 30 m/s for 100 steps:
Total = -0.0015 Ã— 100 = -0.15  (noticeable but small)
```

**Effect:** Gentle discouragement of reckless speed (harder to control = more crashes)

---

## ğŸ¯ Complete Reward Examples

### **Example 1: Perfect Navigation**
```
Scenario:
â”œâ”€â”€ Reach goal in 15 seconds
â”œâ”€â”€ Arrival speed: 2.5 m/s (safe!)
â”œâ”€â”€ No crashes, smooth flight

Rewards:
â”œâ”€â”€ Goal: +2.0 (speed â‰¤ 3.0 m/s, no penalty)
â”œâ”€â”€ Time: -0.001 Ã— ~750 steps = -0.75
â”œâ”€â”€ Speed: ~0 (stayed under 15 m/s)
â””â”€â”€ TOTAL: +2.0 - 0.75 = +1.25 âœ… Positive!
```

### **Example 2: Fast But Controlled**
```
Scenario:
â”œâ”€â”€ Reach goal in 10 seconds (fast!)
â”œâ”€â”€ Arrival speed: 8.0 m/s (too fast)
â”œâ”€â”€ Went 20 m/s for ~200 steps mid-flight

Rewards:
â”œâ”€â”€ Goal base: +2.0
â”œâ”€â”€ Speed penalty: (8.0 - 3.0) Ã— 0.1 = -0.5
â”œâ”€â”€ Goal total: +2.0 - 0.5 = +1.5
â”œâ”€â”€ Time: -0.001 Ã— ~500 = -0.5
â”œâ”€â”€ Excessive speed: -0.0001 Ã— 5 Ã— 200 = -0.1
â””â”€â”€ TOTAL: +1.5 - 0.5 - 0.1 = +0.9 âœ… Still positive but less
```

### **Example 3: Crashed**
```
Scenario:
â”œâ”€â”€ Crashed into asteroid at 8 seconds
â”œâ”€â”€ Was going 25 m/s when crashed

Rewards:
â”œâ”€â”€ Collision: -1.0
â”œâ”€â”€ Time: -0.001 Ã— ~400 = -0.4
â”œâ”€â”€ Excessive speed: -0.0001 Ã— 10 Ã— 400 = -0.4
â””â”€â”€ TOTAL: -1.0 - 0.4 - 0.4 = -1.8 âŒ Negative
```

### **Example 4: Timeout (Circling)**
```
Scenario:
â”œâ”€â”€ Timeout at 30 seconds
â”œâ”€â”€ Stayed under 15 m/s (safe but slow)
â”œâ”€â”€ Never reached goal

Rewards:
â”œâ”€â”€ Timeout: -1.0
â”œâ”€â”€ Time: -0.001 Ã— ~1500 = -1.5
â””â”€â”€ TOTAL: -1.0 - 1.5 = -2.5 âŒ Very negative!

Worse than crashing! Agent learns: "Do something, don't circle!"
```

### **Example 5: Left Boundary**
```
Scenario:
â”œâ”€â”€ Left boundary at 12 seconds
â”œâ”€â”€ Was thrusting hard, went 35 m/s

Rewards:
â”œâ”€â”€ Boundary: -1.0
â”œâ”€â”€ Time: -0.001 Ã— ~600 = -0.6
â”œâ”€â”€ Excessive speed: -0.0001 Ã— 20 Ã— 600 = -1.2
â””â”€â”€ TOTAL: -1.0 - 0.6 - 1.2 = -2.8 âŒ Very bad
```

---

## ğŸ¯ Reward Comparison Table

| Outcome | Goal | Time | Speed | Timeout | Failure | **TOTAL** |
|---------|------|------|-------|---------|---------|-----------|
| **Perfect (slow, safe)** | +2.0 | -0.75 | 0 | 0 | 0 | **+1.25** âœ… |
| **Perfect (fast, safe)** | +2.0 | -0.5 | -0.1 | 0 | 0 | **+1.4** âœ… Best! |
| **Goal (too fast)** | +1.5 | -0.5 | -0.1 | 0 | 0 | **+0.9** âœ… OK |
| **Crashed** | 0 | -0.4 | -0.4 | 0 | -1.0 | **-1.8** âŒ |
| **Timeout (circling)** | 0 | -1.5 | 0 | -1.0 | 0 | **-2.5** âŒ Worst! |
| **Boundary** | 0 | -0.6 | -1.2 | 0 | -1.0 | **-2.8** âŒ |

**Key Insight:** Timeout is now worse than crashing! Agent must take action.

---

## ğŸ§  Expected Learning Behavior

### **Phase 1: Random Exploration (0-50k)**
```
Behavior:
â”œâ”€â”€ Random thrusting/rotation
â”œâ”€â”€ Many crashes and timeouts
â””â”€â”€ Occasional lucky goal reaches

Rewards:
â”œâ”€â”€ Mean: -2.0 to -3.0 (very negative)
â””â”€â”€ Std: High (inconsistent)
```

### **Phase 2: Basic Navigation (50k-200k)**
```
Behavior:
â”œâ”€â”€ Starts rotating toward goal
â”œâ”€â”€ Still crashes often
â”œâ”€â”€ Some overshoots (too fast)
â””â”€â”€ Fewer timeouts (taking action!)

Rewards:
â”œâ”€â”€ Mean: -1.0 to 0 (improving)
â””â”€â”€ Occasional positive spikes (goal reaches)
```

### **Phase 3: Controlled Movement (200k-500k)**
```
Behavior:
â”œâ”€â”€ Smoother rotation
â”œâ”€â”€ Thrust + rotation coordination
â”œâ”€â”€ Some speed control
â””â”€â”€ 20-40% success rate

Rewards:
â”œâ”€â”€ Mean: 0 to +0.5 (positive trend!)
â””â”€â”€ More consistent positive rewards
```

### **Phase 4: Optimized Navigation (500k-2M+)**
```
Behavior:
â”œâ”€â”€ Efficient flip-and-burn
â”œâ”€â”€ Smooth deceleration
â”œâ”€â”€ Safe arrival speeds (<3 m/s)
â”œâ”€â”€ Good obstacle avoidance
â””â”€â”€ 60-80%+ success rate

Rewards:
â”œâ”€â”€ Mean: +0.5 to +1.0 (consistently positive!)
â””â”€â”€ Low std (stable policy)
```

---

## âš™ï¸ Tuning the Rewards

### **If Agent Still Circles (Not Goal-Seeking):**

**Increase goal reward:**
```
Goal Reward: 3.0  â† Make goal even more attractive
Timeout Penalty: -1.5  â† Make timeout even worse
```

**Or decrease time penalty:**
```
Time Step Penalty: -0.0005  â† Less penalty for taking time
```

---

### **If Agent Crashes Too Often (Too Aggressive):**

**Increase collision penalty:**
```
Collision Penalty: -1.5  â† Make crashes more painful
```

**Or add distance-to-goal shaping (advanced):**
```csharp
// In OnActionReceived, after applying forces:
float distanceToGoal = Vector3.Distance(transform.position, goalTransform.position);
float previousDistance = ... // Store from last step
if (distanceToGoal < previousDistance)
{
    AddReward(0.0001f);  // Small reward for getting closer
}
```

---

### **If Agent Arrives Too Fast (Unsafe):**

**Increase speed penalties:**
```
Speed Penalty Multiplier: 0.2  â† Double the penalty
Max Speed At Goal: 2.0  â† Lower safe speed threshold
```

**Or add stronger excessive speed penalty:**
```
Excessive Speed Penalty: 0.0005  â† 5Ã— stronger during flight
```

---

### **If Agent Is Too Slow (Conservative):**

**Reduce speed penalties:**
```
Speed Penalty Multiplier: 0.05  â† Halve the penalty
Max Speed At Goal: 5.0  â† Allow faster arrivals
Excessive Speed Penalty: 0.00005  â† Weaken flight penalty
```

**Or increase timeout pressure:**
```
Max Episode Time: 20  â† Shorter episodes
Timeout Penalty: -1.5  â† Harsher timeout
```

---

## ğŸ® Recommended Starting Values (Current Setup)

```
CubeSat Agent â†’ Inspector â†’ Reward Settings:

âœ“ Goal Reward: 2.0                 â† 2Ã— better than any failure
âœ“ Collision Penalty: -1.0          â† Equal to other failures
âœ“ Timeout Penalty: -1.0            â† Equal to collision/boundary
âœ“ Time Step Penalty: -0.001        â† Small per-step cost
âœ“ Max Speed At Goal: 3.0           â† Reasonable safe speed
âœ“ Speed Penalty Multiplier: 0.1    â† Moderate speed penalty
âœ“ Excessive Speed Penalty: 0.0001  â† Tiny flight speed penalty

Expected total rewards:
â”œâ”€â”€ Perfect navigation: +1.0 to +1.5 âœ…
â”œâ”€â”€ Fast navigation: +0.5 to +1.0 âœ…
â”œâ”€â”€ Crash: -1.5 to -2.0 âŒ
â”œâ”€â”€ Timeout: -2.0 to -2.5 âŒ
â””â”€â”€ Boundary: -2.0 to -3.0 âŒ
```

---

## ğŸ“Š Monitoring in TensorBoard

### **Key Metrics:**

**Environment/Cumulative Reward:**
```
Target progression:
â”œâ”€â”€ 0-100k steps: -2.0 to -1.0 (learning basics)
â”œâ”€â”€ 100k-500k: -1.0 to 0 (getting better)
â”œâ”€â”€ 500k-1M: 0 to +0.5 (consistent success)
â””â”€â”€ 1M+: +0.5 to +1.0 (optimized) âœ…
```

**Environment/Episode Length:**
```
Target:
â”œâ”€â”€ Early: 300-1000 steps (many timeouts)
â”œâ”€â”€ Mid: 200-500 steps (taking action)
â””â”€â”€ Late: 200-400 steps (efficient navigation)
```

**Policy/Learning Rate:**
```
Should decrease linearly from 3e-4 to 0
```

---

## ğŸ§ª Testing Your Changes

### **Test 1: Quick Manual Test**
```
1. Set Behavior Type: Heuristic Only (for testing)
2. Press Play
3. Reach goal manually (W + rotation)
4. Check Console for final reward
5. Should see: "Reward: +1.5 to +2.0" if slow/safe âœ“
```

### **Test 2: Monitor First Episode**
```
1. Set Behavior Type: Default
2. Start training
3. Press Play in Unity
4. Watch first episode in Scene view
5. Check Console: Early rewards should be very negative (-2 to -3)
```

### **Test 3: Compare to Previous Run**
```
After 100k steps:
â”œâ”€â”€ Old mean reward: -1.5 to -2.0 (circling)
â””â”€â”€ New mean reward: Should be better (-1.0 to -0.5) âœ“

After 500k steps:
â”œâ”€â”€ Old: Still negative (not goal-seeking)
â””â”€â”€ New: Should be positive or near 0 (goal-seeking!) âœ“
```

---

## ğŸ¯ Success Criteria

**Training is working well when:**

1. **Mean Reward Trends Upward**
   - 100k: -1.5 to -1.0
   - 300k: -0.5 to 0
   - 500k: 0 to +0.5
   - 1M+: +0.5 to +1.0 âœ…

2. **Agent Behavior Shows:**
   - âœ… Active goal-seeking (not circling)
   - âœ… Rotation toward goal
   - âœ… Thrust + rotation coordination
   - âœ… Deceleration near goal (flip-and-burn)
   - âœ… Safe arrival speeds (<3-5 m/s)

3. **Episode Outcomes:**
   - âœ… Timeout rate decreases (<20% by 500k steps)
   - âœ… Goal reach rate increases (>50% by 1M steps)
   - âœ… Crash rate moderate (20-30% acceptable)

---

## ğŸš€ Summary of Changes

### **What Changed:**

```
OLD Reward Structure:
â”œâ”€â”€ Goal: +1.0
â”œâ”€â”€ Collision: -1.0
â”œâ”€â”€ Boundary: -1.0
â”œâ”€â”€ Timeout: -0.5  â† Too lenient!
â”œâ”€â”€ Time: -0.001 per step
â””â”€â”€ Problem: Circling = less bad than trying

NEW Reward Structure:
â”œâ”€â”€ Goal: +2.0  â† DOUBLED!
â”œâ”€â”€ Collision: -1.0
â”œâ”€â”€ Boundary: -1.0
â”œâ”€â”€ Timeout: -1.0  â† Equal to failures!
â”œâ”€â”€ Time: -0.001 per step
â”œâ”€â”€ Speed at goal: 0 to -1.7  â† Encourages safe arrival!
â””â”€â”€ Excessive speed: Small penalty  â† Discourages recklessness
```

### **Expected Effect:**

```
Before:
â”œâ”€â”€ Agent circles, avoids asteroids
â”œâ”€â”€ Rarely reaches goal
â””â”€â”€ Learns "survival > success"

After:
â”œâ”€â”€ Agent seeks goal actively
â”œâ”€â”€ Balances speed vs. safety
â”œâ”€â”€ Learns "controlled success > survival"
â””â”€â”€ Develops flip-and-burn maneuvers âœ…
```

---

## ğŸ“ Advanced: Curriculum Learning (Optional)

**If training is still difficult, try progressive difficulty:**

### **Stage 1: Easy (0-500k steps)**
```
CubeSat Agent:
â”œâ”€â”€ Min Goal Distance: 15  â† Closer goals
â”œâ”€â”€ Max Episode Time: 40  â† More time

AsteroidSpawner:
â”œâ”€â”€ Asteroid Count: 10  â† Fewer obstacles
â”œâ”€â”€ Max Drift Speed: 1.0  â† Slower asteroids
```

### **Stage 2: Medium (500k-1.5M steps)**
```
CubeSat Agent:
â”œâ”€â”€ Min Goal Distance: 20
â”œâ”€â”€ Max Episode Time: 30

AsteroidSpawner:
â”œâ”€â”€ Asteroid Count: 20
â”œâ”€â”€ Max Drift Speed: 2.0
```

### **Stage 3: Hard (1.5M+ steps)**
```
CubeSat Agent:
â”œâ”€â”€ Min Goal Distance: 25  â† Current setting
â”œâ”€â”€ Max Episode Time: 30

AsteroidSpawner:
â”œâ”€â”€ Asteroid Count: 30
â”œâ”€â”€ Max Drift Speed: 3.0
```

---

**Your reward structure is now balanced! Time to retrain and see goal-seeking behavior!** ğŸš€âœ¨

**Key improvement:** Timeout penalty now equals failure penalties â†’ Agent learns to ACT instead of SURVIVE!
