# âœ… Pre-Training Checklist - CubeSat Agent

## ğŸš¨ CRITICAL: Change Behavior Type Before Training!

### **âš ï¸ YOU MUST DO THIS:**

1. **Select CubeSat** in Hierarchy
2. **Find Behavior Parameters** component in Inspector
3. **Change Behavior Type:**
   ```
   Behavior Type: Default  â† Change from "Heuristic Only"!
   ```
4. **Save Scene** (Ctrl+S / Cmd+S)

**WHY:** 
- `Heuristic Only` = manual keyboard control (for testing)
- `Default` = AI training mode (required for ML-Agents)

---

## ğŸ“‹ Complete Pre-Training Checklist

### **1. BehaviorParameters Settings** âš ï¸ CRITICAL

```
Select: CubeSat â†’ Inspector â†’ Behavior Parameters

âœ“ Behavior Type: Default  â† MUST BE "Default" for training!
âœ“ Behavior Name: CubeSat
âœ“ Vector Observation
  â”œâ”€â”€ Space Size: 25
  â””â”€â”€ Stacked Vectors: 1
âœ“ Actions
  â”œâ”€â”€ Continuous Actions: 4
  â””â”€â”€ Discrete Branches: Size 0
```

**Verify:**
- [ ] Behavior Type is **Default** (not Heuristic Only!)
- [ ] Space Size is **25** (not 16)
- [ ] Continuous Actions is **4** (not 3)

---

### **2. CubeSat Agent Settings**

```
Select: CubeSat â†’ Inspector â†’ CubeSat Agent

Physics:
â”œâ”€â”€ Max Thrust: 10
â”œâ”€â”€ Max Torque: 2
â””â”€â”€ Agent Mass: 1

Episode:
â””â”€â”€ Max Episode Time: 30

Goal:
â”œâ”€â”€ Goal Reach Distance: 2
â””â”€â”€ Goal Transform: /Goal âœ“

Boundary:
â”œâ”€â”€ Enforce Boundary: â˜‘
â”œâ”€â”€ Boundary Size: (80, 80, 80)
â””â”€â”€ Boundary Penalty: -1

Spawn Settings:
â”œâ”€â”€ Randomize Start Position: â˜‘
â”œâ”€â”€ Start Spawn Area Size: (20, 20, 20)
â”œâ”€â”€ Randomize Goal Position: â˜‘
â”œâ”€â”€ Goal Spawn Area Size: (80, 80, 80)
â””â”€â”€ Min Goal Distance: 25

References:
â”œâ”€â”€ Asteroid Spawner: /AsteroidSpawner âœ“
â””â”€â”€ Telemetry UI: /TelemetryManager âœ“
```

**Verify:**
- [ ] Goal Transform is assigned
- [ ] Asteroid Spawner is assigned
- [ ] Randomize Start/Goal are checked

---

### **3. Scene Setup**

```
Hierarchy Check:
â”œâ”€â”€ CubeSat âœ“
â”œâ”€â”€ Goal âœ“
â”œâ”€â”€ AsteroidSpawner âœ“
â”œâ”€â”€ TelemetryManager âœ“
â”œâ”€â”€ Arena âœ“
â””â”€â”€ Main Camera âœ“
```

**Verify:**
- [ ] All GameObjects exist in scene
- [ ] Scene is saved

---

### **4. Training Configuration**

**File:** `/Assets/ML-Agents/CubeSat.yaml`

```yaml
behaviors:
  CubeSat:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048         # Increased for better stability
      buffer_size: 20480       # Increased for complex task
      learning_rate: 3.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true          # âœ… Enabled (important!)
      hidden_units: 256
      num_layers: 3            # Increased for rotation+thrust
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 3000000         # 3M steps (rotation is harder)
    time_horizon: 128          # Longer horizon for planning
    summary_freq: 10000
    keep_checkpoints: 5
    checkpoint_interval: 100000
```

**Key Changes for Realistic Physics:**
- âœ… **normalize: true** (helps with varied observations)
- âœ… **num_layers: 3** (deeper network for rotation coordination)
- âœ… **time_horizon: 128** (longer for multi-step maneuvers)
- âœ… **max_steps: 3M** (more training needed)
- âœ… **batch_size: 2048** (larger batches for stability)

**Verify:**
- [ ] File exists at `/Assets/ML-Agents/CubeSat.yaml`
- [ ] `normalize: true` is set

---

### **5. Environment Validation**

**Test in Play Mode:**

1. **Press Play** â–¶ï¸
2. **Check randomization:**
   - [ ] CubeSat starts in different positions
   - [ ] Goal appears in different locations
   - [ ] Asteroids are randomized
3. **Check telemetry:**
   - [ ] Velocity updates
   - [ ] Angular Velocity updates (shows values when rotating)
   - [ ] CubeSat Pos updates
   - [ ] Goal Pos displays correctly
4. **Check boundaries:**
   - [ ] CubeSat can move in full 3D (including -Y)
   - [ ] Episode ends if leaving boundary
5. **Stop Play**

---

### **6. Training Command**

**Open Terminal/Command Prompt:**

```bash
# Navigate to your project root
cd /path/to/pathfinding_1

# Start training
mlagents-learn Assets/ML-Agents/CubeSat.yaml --run-id=CubeSat_Realistic_v1

# When prompted "Start training by pressing...", click Play â–¶ï¸ in Unity
```

**Important:**
- Run command from **project root directory**
- Use `--run-id` to name this training run
- Press Play in Unity **after** seeing the prompt
- Keep Unity in focus initially to ensure connection

---

### **7. Expected Training Behavior**

#### **Early Training (0-100k steps):**
```
Behavior:
â”œâ”€â”€ Random rotation
â”œâ”€â”€ Random thrusting
â”œâ”€â”€ Many crashes into asteroids
â”œâ”€â”€ Many boundary violations
â”œâ”€â”€ Rare goal reaching

Metrics:
â”œâ”€â”€ Cumulative Reward: -50 to -200 (very negative)
â”œâ”€â”€ Episode Length: 30-300 steps (many timeouts)
â””â”€â”€ Success Rate: <5%
```

#### **Mid Training (100k-500k steps):**
```
Behavior:
â”œâ”€â”€ Starting to rotate toward goal
â”œâ”€â”€ Some thrust coordination
â”œâ”€â”€ Fewer random crashes
â”œâ”€â”€ Still many overshoots

Metrics:
â”œâ”€â”€ Cumulative Reward: -20 to +20 (improving)
â”œâ”€â”€ Episode Length: 100-400 steps
â””â”€â”€ Success Rate: 10-30%
```

#### **Late Training (500k-2M+ steps):**
```
Behavior:
â”œâ”€â”€ Smooth rotation toward goal
â”œâ”€â”€ Coordinated thrust + rotation
â”œâ”€â”€ Flip-and-burn deceleration
â”œâ”€â”€ Obstacle avoidance
â”œâ”€â”€ Efficient navigation

Metrics:
â”œâ”€â”€ Cumulative Reward: +50 to +200 (positive!)
â”œâ”€â”€ Episode Length: 200-500 steps (efficient)
â””â”€â”€ Success Rate: 60-80%+
```

---

### **8. Monitoring Training**

#### **TensorBoard (Recommended):**
```bash
# In a new terminal
tensorboard --logdir results

# Open browser to: http://localhost:6006
```

**Key Metrics to Watch:**
```
Environment/Cumulative Reward
â”œâ”€â”€ Should trend upward over time
â””â”€â”€ Target: Eventually positive (+50 to +200)

Environment/Episode Length
â”œâ”€â”€ Should stabilize
â””â”€â”€ Target: 200-500 steps

Losses/Policy Loss
â”œâ”€â”€ Should decrease initially
â””â”€â”€ Then stabilize (small oscillations normal)

Policy/Learning Rate
â””â”€â”€ Should decrease linearly to 0
```

#### **Console Output:**
```
Example good progress:
[INFO] Step: 50000.  Time Elapsed: 145.2 s.
Mean Reward: -45.3   â† Improving (was -100)
Std of Reward: 23.4
...
[INFO] Step: 500000. Time Elapsed: 2341.7 s.
Mean Reward: 15.2    â† Positive! Good sign
Std of Reward: 35.6
```

---

### **9. Common Issues & Solutions**

#### **Issue: "No Unity environment detected"**
```
Solution:
1. Make sure Unity is in Play mode
2. Check BehaviorParameters â†’ Behavior Type is "Default"
3. Restart training command and press Play again
```

#### **Issue: "Dimension mismatch error"**
```
Solution:
1. Verify Vector Observation Space Size = 25
2. Verify Continuous Actions = 4
3. Delete old models in results/ folder
4. Restart training
```

#### **Issue: Reward stays very negative**
```
Possible causes:
â”œâ”€â”€ Boundary too small (increase boundarySize)
â”œâ”€â”€ Goal too far (decrease minGoalDistance)
â”œâ”€â”€ Asteroids too dense (decrease asteroid count)
â””â”€â”€ Episode too short (increase maxEpisodeTime)

Try easier settings first, then increase difficulty
```

#### **Issue: Training is very slow**
```
Solutions:
â”œâ”€â”€ Reduce maxEpisodeTime (30 â†’ 20 seconds)
â”œâ”€â”€ Close other applications
â”œâ”€â”€ Lower asteroid count in AsteroidSpawner
â””â”€â”€ Reduce Time Scale in Unity (Edit â†’ Project Settings â†’ Time)
```

---

### **10. Training Configuration Variants**

#### **Fast Training (Easier, quicker results):**
```yaml
# CubeSat.yaml adjustments:
max_steps: 1500000        # Fewer steps needed
time_horizon: 64          # Shorter horizon
```

**In CubeSat Agent:**
```
Max Episode Time: 20      # Faster episodes
Min Goal Distance: 15     # Closer goals
Asteroid Count: 15        # Fewer obstacles (in AsteroidSpawner)
```

#### **Production Training (Harder, better policy):**
```yaml
# CubeSat.yaml (current settings):
max_steps: 3000000        # More steps for complex task
time_horizon: 128         # Longer planning horizon
```

**In CubeSat Agent:**
```
Max Episode Time: 30      # Longer episodes
Min Goal Distance: 25     # Farther goals
Asteroid Count: 30        # More obstacles
Add Initial Tumble: â˜‘     # Start with rotation
```

---

### **11. Success Criteria**

**Consider training successful when:**
- [ ] **Cumulative Reward** averaging **+50 or higher**
- [ ] **Success Rate** (goal reaching) above **60%**
- [ ] **Episode Length** averaging **200-400 steps**
- [ ] Agent shows **smooth rotation + thrust** coordination
- [ ] Agent demonstrates **flip-and-burn** deceleration
- [ ] Training has run for at least **1-2M steps**

**At this point:**
- Export the trained model
- Set Behavior Type back to "Inference Only"
- Assign the trained model to the Model field
- Test in different scenarios

---

## ğŸš€ Final Pre-Training Steps (DO THIS NOW!)

### **STEP 1: Change Behavior Type**
```
1. Select CubeSat in Hierarchy
2. Inspector â†’ Behavior Parameters
3. Behavior Type: Default â—„â”€â”€ CHANGE THIS NOW!
4. Save Scene (Ctrl+S)
```

### **STEP 2: Verify Settings**
```
- [ ] Behavior Type = Default
- [ ] Space Size = 25
- [ ] Continuous Actions = 4
- [ ] Goal assigned
- [ ] Asteroids assigned
```

### **STEP 3: Test One Last Time**
```
1. Press Play
2. Check randomization works
3. Check telemetry displays correctly
4. Stop Play
```

### **STEP 4: Start Training**
```bash
# Terminal:
mlagents-learn Assets/ML-Agents/CubeSat.yaml --run-id=CubeSat_Realistic_v1

# Unity: Press Play when prompted
```

### **STEP 5: Open TensorBoard**
```bash
# New terminal:
tensorboard --logdir results

# Browser: http://localhost:6006
```

---

## ğŸ“Š Quick Reference Card

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            CUBESAT TRAINING - QUICK REFERENCE            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                          â•‘
â•‘  CRITICAL SETTINGS:                                      â•‘
â•‘  â”œâ”€â”€ Behavior Type: Default (not Heuristic Only!)       â•‘
â•‘  â”œâ”€â”€ Space Size: 25                                      â•‘
â•‘  â””â”€â”€ Continuous Actions: 4                               â•‘
â•‘                                                          â•‘
â•‘  TRAINING COMMAND:                                       â•‘
â•‘  mlagents-learn Assets/ML-Agents/CubeSat.yaml \          â•‘
â•‘    --run-id=CubeSat_Realistic_v1                         â•‘
â•‘                                                          â•‘
â•‘  EXPECTED DURATION:                                      â•‘
â•‘  â”œâ”€â”€ Fast results: 30-60 minutes (1M steps)              â•‘
â•‘  â”œâ”€â”€ Good results: 1-2 hours (2M steps)                  â•‘
â•‘  â””â”€â”€ Best results: 3-4 hours (3M steps)                  â•‘
â•‘                                                          â•‘
â•‘  SUCCESS METRICS:                                        â•‘
â•‘  â”œâ”€â”€ Reward: +50 to +200                                 â•‘
â•‘  â”œâ”€â”€ Success Rate: 60%+                                  â•‘
â•‘  â””â”€â”€ Episode Length: 200-400 steps                       â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## âœ… You're Ready When:

- âœ… Behavior Type is **Default**
- âœ… Vector Observation Space Size is **25**
- âœ… Continuous Actions is **4**
- âœ… CubeSat.yaml exists and is configured
- âœ… Environment randomization works
- âœ… Telemetry displays correctly
- âœ… Scene is saved

**NOW GO TRAIN!** ğŸš€ğŸ¤–âœ¨

Good luck! Training a realistic spacecraft controller is challenging but rewarding!
